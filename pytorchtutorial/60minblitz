http://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html
## Tensor
torch --> numpy : VAR.numpy()
numpy --> torch : torch.from_numpy(VAR)
cuda : VAR = VAR.cuda()
## Variable
autograd package --> automatic differentiation for all operation on Tensors
i) autograd.Variable is central class of the package, wrapping a Tensor; Variable[data,grad,creator]
ii) function; Variable has .grad_fn attribute, that refenrences a function, that created the variable 
    (variables created by the user has .grad_fn == None)
 .backward() calculates the derivatives, if variable has more then one element, it has to be specified
 ## neural networks
 uses torch.nn package
 nn.Module - encapsulating parameters to module
 nn.Parameter - kind of var automatically registered
 autograd.Function - implements forward and backward definition of an autograd
 
 ## cifar example --> see ct1.py
 
 ## Training on GPU
 net.cuda()
 inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())
 
